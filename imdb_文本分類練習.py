# -*- coding: utf-8 -*-
"""IMDB 文本分類練習.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hSgpmKoJT9O7edhYxrIin-hCC5yCnf8_
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformers
# %pip install torch

# 技術方法
# 詞袋模型 (Bag-of-Words)
# 詞袋模型將文本文件視為詞語的集合，忽略語法和詞序。它通過計算每個詞在文本中的出現頻率，創建一個數值特徵向量。
# 詞頻-逆文檔頻率 (TF-IDF)
# TF-IDF通過考慮詞在文檔中的出現頻率以及在整個語料庫中的逆頻率，表示詞在文檔中的重要性。
# 詞嵌入 (Word Embedding)
# 詞嵌入技術 (如Word2Vec和GloVe) 將詞表示為高維向量空間中的密集向量，捕捉詞之間的語義關係。
# 機器學習算法
# 可以使用支持向量機 (SVM)、朴素貝葉斯、決策樹等監督學習算法進行文本分類，利用提取的特徵。
# 深度學習模型
# 深度學習模型，如卷積神經網絡 (CNN)、循環神經網絡 (RNN)和基於Transformer的模型 (如BERT)，在文本分類任務中取得了顯著進展。

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel
from transformers import AdamW
from tqdm import tqdm

#%% Loader data

# =============================================================================
# # 加載 IMDB 數據集
# dataset = load_dataset('imdb')
# dataset["train"][0]['text']
# =============================================================================

import re

def preprocess_text(text):
    # 替换 <br /> 为句号或空格
    text = re.sub(r"<br\s*/?>", ". ", text)  # 或者替换为空格
    text = re.sub(r"\s+", " ", text.strip())  # 清理多余空格
    return text

data = pd.read_csv(r"/content/IMDB Dataset.csv")

data["review"] = data["review"].apply(preprocess_text)
data["sentiment"] = data["sentiment"].apply(lambda x: 0 if x == "positive" else 1)
train, test = train_test_split(data[0:25000], test_size=0.2)

train, test = train.reset_index(drop=True), test.reset_index(drop=True)

train["review"][0]

train["review"][0]
len(train)
type(train["sentiment"][0])

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
inputs = tokenizer("Hello, how are you?", return_tensors="pt") #"pt" means return pytorch tensor
outputs = model(**inputs)
outputs.logits

inputs

from transformers import BertTokenizer

# 初始化 BERT tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# 输入文本列表
texts = ["This is an example sentence.", "Another example sentence for tokenization."]

# 使用 tokenizer 进行标记化并填充
tokens = tokenizer(
    texts,                      # 输入文本
    padding=True,                # 填充样本
    truncation=True,             # 超过 max_length 截断
    max_length=512,              # 最大长度限制
    return_tensors="pt"          # 返回 PyTorch 张量
)

# 输出填充后的数据
input_ids = tokens["input_ids"]    # Token IDs shape: (batch_size, seq_length)
attention_mask = tokens["attention_mask"]  # Attention mask shape: (batch_size, seq_length)

print("Input IDs shape:", input_ids.shape)
print("Attention Mask shape:", attention_mask.shape)

padded = tokenizer.pad(
    {"input_ids": input_ids, "attention_mask": attention_mask},
    padding=True,
    return_tensors="pt",
)

print(padded["input_ids"].shape)
print(padded["attention_mask"].shape)

class MovieData(Dataset):
    def __init__(self, data, tokenizer):
        self.X = data["review"]
        self.Y = data["sentiment"]
        self.len = len(self.Y)
        self.tokenizer = tokenizer

    def __len__(self):
        return (self.len)

    def __getitem__(self, index):
        # if index >= self.len:
        #     raise StopAsyncIteration
        # else:
        content = tokenizer(self.X[index], return_tensors="pt", padding="max_length", max_length=512, truncation=True)
        labels = self.Y[index]
        return({
            "input_ids": content["input_ids"].squeeze(0), # 讓其便1維度 以便後續dataloader batch
            "attention_mask": content["attention_mask"].squeeze(0),
            "labels": torch.tensor(labels, dtype=torch.long),
        })

# # 定義 collate_fn
# def collate_fn(batch):
#     input_ids = [item["input_ids"] for item in batch]
#     attention_mask = [item["attention_mask"] for item in batch]
#     labels = [item["labels"] for item in batch]

#     # 動態 padding
#     padded = tokenizer.pad(
#         {"input_ids": input_ids, "attention_mask": attention_mask},
#         padding=True,
#         return_tensors="pt",
#     )

#     return {
#         "input_ids": padded["input_ids"],
#         "attention_mask": padded["attention_mask"],
#         "labels": torch.tensor(labels, dtype=torch.long),
#     }

movie_comment_train = MovieData(train, tokenizer)
movie_comment_test = MovieData(test, tokenizer)
len(movie_comment_train)

batch_size = 16
# dataloader_train = iter(DataLoader(movie_comment_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn))
dataloader_train = DataLoader(movie_comment_train, batch_size=batch_size)#, shuffle=True # 注意如果DataLoder 前面加iter 會讓之後訓練跑迴圈第一次跑完 全部就變空的，會有異常
test_loader = DataLoader(movie_comment_test, batch_size=16)
# input_ = next(dataloader_train)
# input_['input_ids'].shape



# outputs = model(**input_)

# 設置設備
# model.to(device) 的具體功能包括：

# 遷移模型的參數與頂點：
# 將模型的重度、偏壓和其他參數從原來的裝置（通常是CPU）移動到目標裝置。
# 支援硬體加速：
# 如果將模型移到GPU（cuda），模型的運算將使用GPU資源，顯著加速訓練和推理。

device0 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device0)

# outputs = model(input_ids, attention_mask=attention_mask, labels=labels)#, labels=labels

# loss = outputs.loss

# 為什麼transformer 可以這樣定義 loss function

# 而不像 一般torch 要定義 loss = nn.BCELoss() losses = loss(pre, act)
# ChatGPT 說：
# ChatGPT
# 在 Transformers 中，输出的 loss 通常是 模型（如 BERT、GPT）内部定义好的，这通常是由于 模型（例如 AutoModelForSequenceClassification）已将损失函数集成到输出中，方便用户直接使用。

# Transformers 模型为什么可以这样定义 loss？
# 在许多 Transformers 模型（如 AutoModelForSequenceClassification）中，损失函数已经被定义为模型的一部分，因此你不需要像一般 PyTorch 模型那样显式地初始化一个 nn.BCELoss()。模型输出通常会直接包含：

# loss：模型内部计算好的损失。
# logits：模型的输出结果，即未归一化的预测值。
# 与一般 PyTorch 模型的区别
# 在一般 PyTorch 模型中：

# 你需要显式地选择一个 loss 类，比如 nn.BCELoss()，然后将 pre（预测值）和 act（实际值）传入进行计算：
# python
# 複製
# 編輯
# loss = nn.BCELoss()(pre, act)
# 在 Transformers 中，由于模型本身集成了损失函数，你只需从模型的输出中提取 loss，模型会自动计算损失。
# Transformers 模型输出示例
# python
# 複製
# 編輯
# outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
# loss = outputs.loss  # 直接从模型输出提取的损失
# logits = outputs.logits  # 模型输出的 logits
# 通常 BERT 分类任务
# 在 BERT 分类任务中，输出的 logits 是模型的未归一化预测值。
# 损失通常是使用 nn.CrossEntropyLoss() 或其他分类损失函数计算：
# python
# 複製
# 編輯
# loss_fct = nn.CrossEntropyLoss()
# loss = loss_fct(logits, labels)
# 总结
# Transformers 模型（如 BERT）的 loss 已经集成在输出中，因此你不需要像一般 PyTorch 模型那样显式地定义损失函数。
# 如果需要使用特定的损失函数，可以在后处理 logits 后显式地计算损失。比如：
# python
# 複製
# 編輯
# loss_fct = nn.CrossEntropyLoss()
# loss = loss_fct(logits, labels)

epochs = 5
import os
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# lossfcn = nn.BCELoss() # BCR
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
from torch.nn.utils import clip_grad_norm_

# 在每个梯度更新步骤后，裁剪梯度
clip_grad_norm_(model.parameters(), max_norm=1.0)
# for epoch in range(epochs):

#     for x_batch, y_batch in dataloader:
#         y_batch_predict = model(x_batch.float())
#         optim.zero_grad()
#         loss = lossfcn(y_batch_predict, y_batch)

#         loss.backward() # grads = tape.gradient(loss, model.trainable_variables) tensorflow 不像torch loss是一個物件裡面已經儲存 要更新參數，需要額外提供
#         optim.step() # optimizer.apply_gradients(zip(grads[4:], model.trainable_variables[4:]) # 當下梯度 前一次梯度 透過迴圈持續更新初始化函數內參數

# dataset[1][1]
# model(dataset[0][0]) #RuntimeError: mat1 and mat2 shapes cannot be multiplied (20x16 and 320x100)
# model(dataset[1][0].reshape(-1,3, 28, 28))
for epoch in range(epochs):
    total_loss = 0
    model.train()
    for batch in tqdm(dataloader_train):  # train_loader is your DataLoader tqdm進度條
        optimizer.zero_grad()

        input_ids = batch["input_ids"].to(device0)
        # print(input_ids.shape ) # torch.Size([16, 512])
        attention_mask = batch["attention_mask"].to(device0)
        labels = batch["labels"].to(device0)
        # 使用的是 BertForSequenceClassification，默认使用 CrossEntropyLoss，确保标签是整数格式：
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)#, labels=labels
        logits = outputs.logits
        # print("Logits:", logits) # logits output 是 tensor [batch_size, #labels]
        loss = outputs.loss


        # loss = lossfcn(outputs.logits, labels)

        # 反向傳播和優化
        # loss.backward()

        # 反向傳播

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(dataloader_train)
    print(f"Epoch {epoch + 1}, Loss: {avg_loss:.10f}")

# 8. 验证模型
import torch.nn.functional as F

model.eval()
all_preds = []
all_labels = []
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device0)
        attention_mask = batch["attention_mask"].to(device0)
        labels = batch["labels"].to(device0)

        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        # AutoModelForSequenceClassification 会直接输出 logits，即未经过 softmax 的得分。
        # 如果需要概率分布，需要手动调用 softmax 函数进行归一化。
        # F.softmax(outputs.logits, dim=1) 将 logits 转换为概率
        print(F.softmax(outputs.logits, dim=1))
        preds = torch.argmax(logits, dim=-1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# 计算准确率
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(all_labels, all_preds)
print(f"Validation Accuracy: {accuracy:.4f}")

# save whole model
from google.colab import drive
drive.mount('/content/drive')
save_path = '/content/drive/MyDrive/model_all_test_loss0.0485.pth'
# FILE = 'model_all_test_loss0.0485.pt'
torch.save(model, save_path)

# load model
save_path = '/content/drive/MyDrive/model_all_test_loss0.0485.pth'
model = torch.load(save_path)

# # for test_text in test["review"][0]:
# train["review"][155]
# model.eval()
# all_preds = []
# all_labels = []
# with torch.no_grad():
#   inputs = tokenizer(train["review"][1000], return_tensors="pt", padding="max_length", max_length=512, truncation=True)
#   input_ids = inputs["input_ids"].to(device)
#   attention_mask = inputs["attention_mask"].to(device)

#   outputs = model(input_ids, attention_mask=attention_mask)
#   print(outputs.logits)
#   probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)  # 轉換為機率
#   prediction = torch.argmax(outputs.logits, dim=-1)  # 得到預測結果
#   print(prediction)
#   print(train["sentiment"][1000])