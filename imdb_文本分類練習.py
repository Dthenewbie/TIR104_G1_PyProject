# -*- coding: utf-8 -*-
"""IMDB 文本分類練習.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hSgpmKoJT9O7edhYxrIin-hCC5yCnf8_
"""


import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel
from transformers import AdamW
from tqdm import tqdm

#%% Loader data

# =============================================================================
# # 加載 IMDB 數據集
# dataset = load_dataset('imdb')
# dataset["train"][0]['text']
# =============================================================================

import re

def preprocess_text(text):
    # 替换 <br /> 为句号或空格
    text = re.sub(r"<br\s*/?>", ". ", text)  # 或者替换为空格
    text = re.sub(r"\s+", " ", text.strip())  # 清理多余空格
    return text

data = pd.read_csv(r"/content/IMDB Dataset.csv")

data["review"] = data["review"].apply(preprocess_text)
data["sentiment"] = data["sentiment"].apply(lambda x: 0 if x == "positive" else 1)
train, test = train_test_split(data[0:25000], test_size=0.2)

train, test = train.reset_index(drop=True), test.reset_index(drop=True)

train["review"][0]

train["review"][0]
len(train)
type(train["sentiment"][0])

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
inputs = tokenizer("Hello, how are you?", return_tensors="pt") #"pt" means return pytorch tensor
outputs = model(**inputs)
outputs.logits

inputs

from transformers import BertTokenizer

# 初始化 BERT tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# 输入文本列表
texts = ["This is an example sentence.", "Another example sentence for tokenization."]

# 使用 tokenizer 进行标记化并填充
tokens = tokenizer(
    texts,                      # 输入文本
    padding=True,                # 填充样本
    truncation=True,             # 超过 max_length 截断
    max_length=512,              # 最大长度限制
    return_tensors="pt"          # 返回 PyTorch 张量
)

# 输出填充后的数据
input_ids = tokens["input_ids"]    # Token IDs shape: (batch_size, seq_length)
attention_mask = tokens["attention_mask"]  # Attention mask shape: (batch_size, seq_length)

print("Input IDs shape:", input_ids.shape)
print("Attention Mask shape:", attention_mask.shape)

padded = tokenizer.pad(
    {"input_ids": input_ids, "attention_mask": attention_mask},
    padding=True,
    return_tensors="pt",
)

print(padded["input_ids"].shape)
print(padded["attention_mask"].shape)

class MovieData(Dataset):
    def __init__(self, data, tokenizer):
        self.X = data["review"]
        self.Y = data["sentiment"]
        self.len = len(self.Y)
        self.tokenizer = tokenizer

    def __len__(self):
        return (self.len)

    def __getitem__(self, index):
        # if index >= self.len:
        #     raise StopAsyncIteration
        # else:
        content = tokenizer(self.X[index], return_tensors="pt", padding="max_length", max_length=512, truncation=True)
        labels = self.Y[index]
        return({
            "input_ids": content["input_ids"].squeeze(0), # 讓其便1維度 以便後續dataloader batch
            "attention_mask": content["attention_mask"].squeeze(0),
            "labels": torch.tensor(labels, dtype=torch.long),
        })



movie_comment_train = MovieData(train, tokenizer)
movie_comment_test = MovieData(test, tokenizer)
len(movie_comment_train)

batch_size = 16
dataloader_train = DataLoader(movie_comment_train, batch_size=batch_size)#, shuffle=True # 注意如果DataLoder 前面加iter 會讓之後訓練跑迴圈第一次跑完 全部就變空的，會有異常
test_loader = DataLoader(movie_comment_test, batch_size=16)


device0 = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device0)

epochs = 5
import os
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# lossfcn = nn.BCELoss() # BCR
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
from torch.nn.utils import clip_grad_norm_

# 在每个梯度更新步骤后，裁剪梯度
clip_grad_norm_(model.parameters(), max_norm=1.0)

for epoch in range(epochs):
    total_loss = 0
    model.train()
    for batch in tqdm(dataloader_train):  # train_loader is your DataLoader tqdm進度條
        optimizer.zero_grad()

        input_ids = batch["input_ids"].to(device0)
        # print(input_ids.shape ) # torch.Size([16, 512])
        attention_mask = batch["attention_mask"].to(device0)
        labels = batch["labels"].to(device0)
        # 使用的是 BertForSequenceClassification，默认使用 CrossEntropyLoss，确保标签是整数格式：
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)#, labels=labels
        logits = outputs.logits
        # print("Logits:", logits) # logits output 是 tensor [batch_size, #labels]
        loss = outputs.loss


        # loss = lossfcn(outputs.logits, labels)

        # 反向傳播和優化
        # loss.backward()

        # 反向傳播

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(dataloader_train)
    print(f"Epoch {epoch + 1}, Loss: {avg_loss:.10f}")

# 8. 验证模型
import torch.nn.functional as F

model.eval()
all_preds = []
all_labels = []
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device0)
        attention_mask = batch["attention_mask"].to(device0)
        labels = batch["labels"].to(device0)

        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        # AutoModelForSequenceClassification 会直接输出 logits，即未经过 softmax 的得分。
        # 如果需要概率分布，需要手动调用 softmax 函数进行归一化。
        # F.softmax(outputs.logits, dim=1) 将 logits 转换为概率
        print(F.softmax(outputs.logits, dim=1))
        preds = torch.argmax(logits, dim=-1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# 计算准确率
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(all_labels, all_preds)
print(f"Validation Accuracy: {accuracy:.4f}")

# save whole model
from google.colab import drive
drive.mount('/content/drive')
save_path = '/content/drive/MyDrive/model_all_test_loss0.0485.pth'
# FILE = 'model_all_test_loss0.0485.pt'
torch.save(model, save_path)

# load model
save_path = '/content/drive/MyDrive/model_all_test_loss0.0485.pth'
model = torch.load(save_path)
